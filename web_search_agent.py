import os
import requests
import json
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import google.generativeai as genai
from dotenv import load_dotenv
import logging

# Load environment variables
load_dotenv()

class WebSearchAgent:
    def __init__(self):
        """Web arama agent'Ä±nÄ± baÅŸlat"""
        self.gemini_api_key = os.getenv('GEMINI_API_KEY') or os.getenv('GEMINI_API_KEY')
        self.serpapi_key = os.getenv('SERPAPI_KEY')  # SerpAPI anahtarÄ±
        
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
            print("âœ… Web Search Agent - Gemini API baÄŸlantÄ±sÄ± kuruldu")
        else:
            print("âš ï¸ Web Search Agent - Gemini API anahtarÄ± bulunamadÄ±")
            self.gemini_model = None
        
        if self.serpapi_key:
            print("âœ… Web Search Agent - SerpAPI baÄŸlantÄ±sÄ± kuruldu")
        else:
            print("âš ï¸ Web Search Agent - SerpAPI anahtarÄ± bulunamadÄ±, basit arama kullanÄ±lacak")
        
        # Arama motorlarÄ±
        self.search_engines = {
            'google': 'https://www.google.com/search',
            'bing': 'https://www.bing.com/search',
            'duckduckgo': 'https://duckduckgo.com/'
        }
        
        # Finansal haber kaynaklarÄ±
        self.financial_sources = [
            'bloomberg.com',
            'reuters.com',
            'cnbc.com',
            'marketwatch.com',
            'yahoo.com/finance',
            'investing.com',
            'tradingview.com',
            'forexfactory.com',
            'fxstreet.com',
            'finans.mynet.com',
            'bloomberght.com',
            'dunya.com',
            'hurriyet.com.tr/ekonomi',
            'milliyet.com.tr/ekonomi',
            'sozcu.com.tr/ekonomi',
            'haberturk.com/ekonomi'
        ]
        
        # KCHOL ile ilgili anahtar kelimeler
        self.kchol_keywords = [
            'KCHOL',
            'KoÃ§ Holding',
            'ArÃ§elik',
            'TofaÅŸ',
            'Ford Otosan',
            'YapÄ± Kredi',
            'KoÃ§ Group',
            'KoÃ§ Holding hisse',
            'KCHOL hisse senedi',
            'KoÃ§ Holding finansal rapor',
            'KoÃ§ Holding Ã§eyreklik rapor',
            'KoÃ§ Holding temettÃ¼',
            'KoÃ§ Holding yatÄ±rÄ±m',
            'KoÃ§ Holding analiz'
        ]
        
        # User agent'lar
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        # Logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def search_web(self, query, max_results=10, search_type='news'):
        """Web'de arama yap"""
        try:
            self.logger.info(f"Web aramasÄ± baÅŸlatÄ±lÄ±yor: {query}")
            
            # Arama sorgusunu optimize et
            optimized_query = self._optimize_search_query(query, search_type)
            
            # SerpAPI varsa kullan
            if self.serpapi_key:
                serp_results = self._search_with_serpapi(optimized_query, max_results, search_type)
                if serp_results:
                    return serp_results
            
            # Fallback: FarklÄ± arama motorlarÄ±nÄ± dene
            results = []
            
            # Google aramasÄ± (basit HTTP isteÄŸi)
            google_results = self._search_google(optimized_query, max_results)
            if google_results:
                results.extend(google_results)
            
            # Bing aramasÄ±
            bing_results = self._search_bing(optimized_query, max_results)
            if bing_results:
                results.extend(bing_results)
            
            # Duplicate'leri temizle
            unique_results = self._remove_duplicates(results)
            
            # SonuÃ§larÄ± filtrele ve sÄ±rala
            filtered_results = self._filter_and_rank_results(unique_results, query)
            
            self.logger.info(f"Toplam {len(filtered_results)} sonuÃ§ bulundu")
            return filtered_results[:max_results]
            
        except Exception as e:
            self.logger.error(f"Web arama hatasÄ±: {e}")
            return []
    
    def _search_with_serpapi(self, query, max_results, search_type):
        """SerpAPI ile arama yap"""
        try:
            if search_type == 'news':
                endpoint = "https://serpapi.com/search.json"
                params = {
                    'engine': 'google_news',
                    'q': query,
                    'api_key': self.serpapi_key,
                    'num': max_results,
                    'hl': 'tr',
                    'gl': 'tr'
                }
            else:
                endpoint = "https://serpapi.com/search.json"
                params = {
                    'engine': 'google',
                    'q': query,
                    'api_key': self.serpapi_key,
                    'num': max_results,
                    'hl': 'tr',
                    'gl': 'tr'
                }
            
            response = requests.get(endpoint, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                if search_type == 'news':
                    articles = data.get('news_results', [])
                else:
                    articles = data.get('organic_results', [])
                
                results = []
                for article in articles:
                    result = {
                        'title': article.get('title', ''),
                        'url': article.get('link', ''),
                        'snippet': article.get('snippet', ''),
                        'source': 'serpapi'
                    }
                    
                    if search_type == 'news':
                        result['published_date'] = article.get('date', '')
                        result['source_name'] = article.get('source', '')
                    
                    results.append(result)
                
                self.logger.info(f"SerpAPI ile {len(results)} sonuÃ§ bulundu")
                return results
            
        except Exception as e:
            self.logger.error(f"SerpAPI arama hatasÄ±: {e}")
        
        return []
    
    def _optimize_search_query(self, query, search_type):
        """Arama sorgusunu optimize et"""
        # KCHOL ile ilgili anahtar kelimeleri ekle
        if 'KCHOL' not in query.upper() and 'koÃ§' not in query.lower():
            query = f"KCHOL {query}"
        
        # Haber aramasÄ± iÃ§in site kÄ±sÄ±tlamasÄ± ekle
        if search_type == 'news':
            financial_sites = ' OR '.join([f'site:{site}' for site in self.financial_sources[:8]])
            query = f'({query}) ({financial_sites})'
        
        # Tarih kÄ±sÄ±tlamasÄ± ekle (son 7 gÃ¼n - daha gÃ¼ncel haberler iÃ§in)
        date_filter = f'after:{datetime.now() - timedelta(days=7):%Y-%m-%d}'
        query = f'{query} {date_filter}'
        
        # GÃ¼ncel haberler iÃ§in ek anahtar kelimeler
        if search_type == 'news':
            current_keywords = ' OR '.join(['bugÃ¼n', 'gÃ¼ncel', 'son dakika', 'son geliÅŸmeler', '2024'])
            query = f'({query}) ({current_keywords})'
        
        return query
    
    def _search_google(self, query, max_results):
        """Google'da arama yap"""
        try:
            headers = {
                'User-Agent': self.user_agents[0],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            params = {
                'q': query,
                'num': max_results,
                'hl': 'tr',
                'gl': 'tr',
                'tbm': 'nws'  # Haber aramasÄ±
            }
            
            response = requests.get(
                self.search_engines['google'],
                params=params,
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                return self._parse_google_results(response.text)
            
        except Exception as e:
            self.logger.error(f"Google arama hatasÄ±: {e}")
        
        return []
    
    def _search_bing(self, query, max_results):
        """Bing'de arama yap"""
        try:
            headers = {
                'User-Agent': self.user_agents[1],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            }
            
            params = {
                'q': query,
                'count': max_results,
                'setlang': 'tr-TR',
                'format': 'rss'
            }
            
            response = requests.get(
                self.search_engines['bing'],
                params=params,
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                return self._parse_bing_results(response.text)
            
        except Exception as e:
            self.logger.error(f"Bing arama hatasÄ±: {e}")
        
        return []
    
    def _parse_google_results(self, html_content):
        """Google sonuÃ§larÄ±nÄ± parse et"""
        results = []
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Haber sonuÃ§larÄ±nÄ± bul
            news_items = soup.find_all('div', class_='g')
            
            for item in news_items:
                try:
                    title_elem = item.find('h3')
                    link_elem = item.find('a')
                    snippet_elem = item.find('div', class_='VwiC3b')
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        url = link_elem.get('href', '')
                        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                        
                        # URL'yi temizle
                        if url.startswith('/url?q='):
                            url = url.split('/url?q=')[1].split('&')[0]
                        
                        if url and title:
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'source': 'google'
                            })
                
                except Exception as e:
                    continue
            
        except Exception as e:
            self.logger.error(f"Google parse hatasÄ±: {e}")
        
        return results
    
    def _parse_bing_results(self, xml_content):
        """Bing sonuÃ§larÄ±nÄ± parse et"""
        results = []
        try:
            soup = BeautifulSoup(xml_content, 'xml')
            
            # RSS item'larÄ±nÄ± bul
            items = soup.find_all('item')
            
            for item in items:
                try:
                    title = item.find('title').get_text(strip=True) if item.find('title') else ''
                    link = item.find('link').get_text(strip=True) if item.find('link') else ''
                    description = item.find('description').get_text(strip=True) if item.find('description') else ''
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'url': link,
                            'snippet': description,
                            'source': 'bing'
                        })
                
                except Exception as e:
                    continue
            
        except Exception as e:
            self.logger.error(f"Bing parse hatasÄ±: {e}")
        
        return results
    
    def _remove_duplicates(self, results):
        """Duplicate sonuÃ§larÄ± temizle"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results
    
    def _filter_and_rank_results(self, results, original_query):
        """SonuÃ§larÄ± filtrele ve sÄ±rala"""
        filtered_results = []
        
        for result in results:
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            url = result.get('url', '').lower()
            
            # KCHOL ile ilgili sonuÃ§larÄ± Ã¶nceliklendir
            score = 0
            
            # Anahtar kelime eÅŸleÅŸmeleri
            if 'kchol' in title or 'koÃ§' in title:
                score += 10
            if 'holding' in title:
                score += 5
            if 'hisse' in title or 'borsa' in title:
                score += 3
            if 'finansal' in title or 'ekonomi' in title:
                score += 2
            
            # URL kalitesi
            if any(source in url for source in self.financial_sources):
                score += 5
            
            # Tarih kontrolÃ¼ (yeni haberler Ã¶ncelikli)
            if '2024' in title or '2024' in snippet:
                score += 2
            
            if score > 0:
                result['relevance_score'] = score
                filtered_results.append(result)
        
        # Skora gÃ¶re sÄ±rala
        filtered_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return filtered_results
    
    def extract_content_from_url(self, url, max_length=2000):
        """URL'den iÃ§erik Ã§Ä±kar"""
        try:
            headers = {
                'User-Agent': self.user_agents[0],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Gereksiz elementleri kaldÄ±r
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # Ana iÃ§eriÄŸi bul
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main',
                '.main-content'
            ]
            
            content = ''
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    break
            
            # EÄŸer Ã¶zel selector bulunamazsa, tÃ¼m metni al
            if not content:
                content = soup.get_text(strip=True)
            
            # Metni temizle ve kÄ±salt
            content = re.sub(r'\s+', ' ', content)
            content = content[:max_length] + '...' if len(content) > max_length else content
            
            return content
            
        except Exception as e:
            self.logger.error(f"Ä°Ã§erik Ã§Ä±karma hatasÄ± ({url}): {e}")
            return None
    
    def analyze_web_content(self, query, search_results):
        """Web iÃ§eriÄŸini Gemini ile analiz et"""
        if not self.gemini_model:
            return "Gemini API kullanÄ±lamÄ±yor."
        
        try:
            # En Ã¶nemli sonuÃ§lardan iÃ§erik Ã§Ä±kar
            content_summary = []
            
            for i, result in enumerate(search_results[:5]):
                title = result.get('title', '')
                url = result.get('url', '')
                snippet = result.get('snippet', '')
                
                # URL'den iÃ§erik Ã§Ä±kar
                full_content = self.extract_content_from_url(url)
                if full_content:
                    content_summary.append(f"Kaynak {i+1}: {title}\nURL: {url}\nÄ°Ã§erik: {full_content[:500]}...\n")
                else:
                    content_summary.append(f"Kaynak {i+1}: {title}\nURL: {url}\nÃ–zet: {snippet}\n")
            
            # Kaynak URL'lerini hazÄ±rla
            source_urls = []
            for i, result in enumerate(search_results[:5]):
                source_urls.append(f"Kaynak {i+1}: {result.get('url', 'N/A')}")
            
            # Gemini ile analiz yap
            analysis_prompt = f"""
Sen bir finans analisti olarak KCHOL hisse senedi ile ilgili web iÃ§eriklerini analiz ediyorsun.

KullanÄ±cÄ± sorusu: {query}

AÅŸaÄŸÄ±daki web iÃ§eriklerini analiz ederek kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla:

{chr(10).join(content_summary)}

Kaynak URL'leri:
{chr(10).join(source_urls)}

Analiz kurallarÄ±:
1. Sadece TÃ¼rkÃ§e yanÄ±t ver
2. Emoji kullanma
3. DÃ¼zyazÄ± ÅŸeklinde yaz
4. KÄ±sa ve Ã¶z ol (maksimum 3-4 paragraf)
5. Teknik jargon kullanma
6. KaynaklarÄ± belirt
7. Haberlerin fiyat Ã¼zerindeki potansiyel etkisini aÃ§Ä±kla
8. Risk uyarÄ±sÄ± ekle

YanÄ±tÄ±nÄ± ver:
"""
            
            response = self.gemini_model.generate_content(analysis_prompt)
            return response.text.strip()
            
        except Exception as e:
            self.logger.error(f"Web iÃ§erik analizi hatasÄ±: {e}")
            return f"Web iÃ§erik analizi yapÄ±lamadÄ±: {str(e)}"
    
    def search_and_analyze(self, query, max_results=10):
        """Web aramasÄ± yap ve analiz et"""
        try:
            self.logger.info(f"Web arama ve analiz baÅŸlatÄ±lÄ±yor: {query}")
            
            # Web aramasÄ± yap
            search_results = self.search_web(query, max_results)
            
            if not search_results:
                return {
                    'success': False,
                    'message': 'Web aramasÄ± sonucu bulunamadÄ±.',
                    'results': [],
                    'analysis': None
                }
            
            # Ä°Ã§eriÄŸi analiz et
            analysis = self.analyze_web_content(query, search_results)
            
            return {
                'success': True,
                'query': query,
                'results_count': len(search_results),
                'results': search_results,
                'analysis': analysis
            }
            
        except Exception as e:
            self.logger.error(f"Web arama ve analiz hatasÄ±: {e}")
            return {
                'success': False,
                'message': f'Web arama hatasÄ±: {str(e)}',
                'results': [],
                'analysis': None
            }
    
    def get_financial_news(self, company='KCHOL', days=7):
        """Finansal haberleri al"""
        try:
            queries = [
                f"{company} hisse senedi haberleri",
                f"{company} finansal rapor",
                f"{company} Ã§eyreklik rapor",
                f"{company} temettÃ¼",
                f"{company} yatÄ±rÄ±m analizi"
            ]
            
            all_results = []
            
            for query in queries:
                results = self.search_web(query, max_results=5, search_type='news')
                all_results.extend(results)
            
            # Duplicate'leri temizle
            unique_results = self._remove_duplicates(all_results)
            
            # SonuÃ§larÄ± sÄ±rala
            ranked_results = self._filter_and_rank_results(unique_results, company)
            
            return {
                'success': True,
                'company': company,
                'results_count': len(ranked_results),
                'results': ranked_results[:10]  # En iyi 10 sonuÃ§
            }
            
        except Exception as e:
            self.logger.error(f"Finansal haber alma hatasÄ±: {e}")
            return {
                'success': False,
                'message': f'Finansal haber alma hatasÄ±: {str(e)}',
                'results': []
            }

    def get_current_news(self, company='KCHOL', max_results=10):
        """GÃ¼ncel haberleri al (son 24-48 saat)"""
        try:
            # GÃ¼ncel haberler iÃ§in Ã¶zel sorgular
            current_queries = [
                f"{company} son dakika",
                f"{company} bugÃ¼n",
                f"{company} gÃ¼ncel",
                f"{company} son geliÅŸmeler",
                f"{company} 2024 son haberler"
            ]
            
            all_results = []
            
            for query in current_queries:
                results = self.search_web(query, max_results=max_results//2, search_type='news')
                all_results.extend(results)
            
            # Duplicate'leri temizle
            unique_results = self._remove_duplicates(all_results)
            
            # SonuÃ§larÄ± sÄ±rala (en yeni Ã¶nce)
            ranked_results = self._filter_and_rank_results(unique_results, company)
            
            # Tarih kontrolÃ¼ yap (son 48 saat)
            current_time = datetime.now()
            recent_results = []
            
            for result in ranked_results:
                # URL'den tarih Ã§Ä±karmaya Ã§alÄ±ÅŸ
                url = result.get('url', '')
                title = result.get('title', '')
                
                # EÄŸer URL'de bugÃ¼n, gÃ¼ncel, son dakika gibi kelimeler varsa Ã¶ncelik ver
                if any(word in url.lower() or word in title.lower() for word in ['bugÃ¼n', 'gÃ¼ncel', 'son dakika', 'son geliÅŸmeler', '2024']):
                    result['relevance_score'] = result.get('relevance_score', 0) + 10
                    recent_results.append(result)
                else:
                    recent_results.append(result)
            
            # Skora gÃ¶re tekrar sÄ±rala
            recent_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            return {
                'success': True,
                'company': company,
                'results_count': len(recent_results),
                'results': recent_results[:max_results],
                'search_type': 'current_news'
            }
            
        except Exception as e:
            self.logger.error(f"GÃ¼ncel haber alma hatasÄ±: {e}")
            return {
                'success': False,
                'message': f'GÃ¼ncel haber alma hatasÄ±: {str(e)}',
                'results': []
            }

    def analyze_price_prediction_with_news(self, user_question, model_prediction, search_query="KCHOL hisse senedi gÃ¼ncel haberler ve analiz"):
        """Model tahmini ve web haberlerini birleÅŸtirerek kapsamlÄ± fiyat analizi yap"""
        try:
            self.logger.info(f"Fiyat tahmini analizi baÅŸlatÄ±lÄ±yor: {user_question}")
            
            # KullanÄ±cÄ± sorusunu analiz et - Ã¶zel durumlar iÃ§in
            is_why_question = any(word in user_question.lower() for word in ['niye', 'neden', 'why', 'sebep', 'gerekÃ§e'])
            is_today_question = any(word in user_question.lower() for word in ['bugÃ¼n', 'today', 'gÃ¼ncel', 'son'])
            is_fall_question = any(word in user_question.lower() for word in ['dÃ¼ÅŸtÃ¼', 'dÃ¼ÅŸÃ¼ÅŸ', 'fall', 'dÃ¼ÅŸer', 'dÃ¼ÅŸecek'])
            
            # Ã–zel sorgular oluÅŸtur
            if is_why_question and is_today_question and is_fall_question:
                # "BugÃ¼n KCHOL niye dÃ¼ÅŸtÃ¼?" sorusu iÃ§in Ã¶zel arama
                search_queries = [
                    "KCHOL hisse senedi bugÃ¼n dÃ¼ÅŸÃ¼ÅŸ nedenleri",
                    "KCHOL KoÃ§ Holding bugÃ¼n neden dÃ¼ÅŸtÃ¼",
                    "KCHOL hisse senedi son dakika haberleri",
                    "KCHOL bugÃ¼n gÃ¼ncel geliÅŸmeler",
                    "KCHOL hisse senedi dÃ¼ÅŸÃ¼ÅŸ analizi bugÃ¼n",
                    "KoÃ§ Holding hisse senedi bugÃ¼nkÃ¼ durum"
                ]
            else:
                # Genel arama sorgularÄ±
                search_queries = [search_query]
            
            # Web'den gÃ¼ncel haberleri ara
            all_search_results = []
            for query in search_queries:
                # Ã–nce gÃ¼ncel haberleri dene
                current_news_result = self.get_current_news('KCHOL', max_results=6)
                if current_news_result.get('success') and current_news_result.get('results'):
                    all_search_results.extend(current_news_result.get('results'))
                    self.logger.info(f"GÃ¼ncel haberler bulundu: {len(current_news_result.get('results'))}")
                
                # Sonra normal arama
                normal_results = self.search_web(query, max_results=6)
                all_search_results.extend(normal_results)
                self.logger.info(f"Normal arama sonuÃ§larÄ±: {len(normal_results)}")
            
            # Duplicate'leri temizle ve sÄ±rala
            unique_results = self._remove_duplicates(all_search_results)
            ranked_results = self._filter_and_rank_results(unique_results, user_question)
            
            if not ranked_results:
                return {
                    'success': False,
                    'message': 'Web aramasÄ± sonucu bulunamadÄ±.',
                    'analysis': None,
                    'model_prediction': model_prediction
                }
            
            # Model tahminini analiz et
            prediction_trend = "yÃ¼kseliÅŸ" if model_prediction.get('change', 0) > 0 else "dÃ¼ÅŸÃ¼ÅŸ" if model_prediction.get('change', 0) < 0 else "stabil"
            prediction_strength = abs(model_prediction.get('change_percent', 0))
            
            # KullanÄ±cÄ± sorusunu analiz et
            user_expects_up = any(word in user_question.lower() for word in ['yÃ¼kselir mi', 'artar mÄ±', 'Ã§Ä±kar mÄ±', 'yukarÄ±'])
            user_expects_down = any(word in user_question.lower() for word in ['dÃ¼ÅŸer mi', 'iner mi', 'aÅŸaÄŸÄ±'])
            
            # Ã‡eliÅŸki analizi
            has_conflict = False
            conflict_explanation = ""
            
            if user_expects_up and prediction_trend == "dÃ¼ÅŸÃ¼ÅŸ":
                has_conflict = True
                conflict_explanation = "KullanÄ±cÄ± yÃ¼kseliÅŸ beklerken model dÃ¼ÅŸÃ¼ÅŸ tahmini yapÄ±yor"
            elif user_expects_down and prediction_trend == "yÃ¼kseliÅŸ":
                has_conflict = True
                conflict_explanation = "KullanÄ±cÄ± dÃ¼ÅŸÃ¼ÅŸ beklerken model yÃ¼kseliÅŸ tahmini yapÄ±yor"
            
            # Web iÃ§eriÄŸini analiz et
            content_summary = []
            source_urls_with_titles = []
            
            for i, result in enumerate(ranked_results[:8]):  # Daha fazla kaynak kullan
                title = result.get('title', '')
                url = result.get('url', '')
                snippet = result.get('snippet', '')
                
                # URL'den iÃ§erik Ã§Ä±kar
                full_content = self.extract_content_from_url(url)
                if full_content:
                    # GÃ¼ncel haberler iÃ§in daha kÄ±sa Ã¶zet
                    content_preview = full_content[:400] if len(full_content) > 400 else full_content
                    content_summary.append(f"GÃ¼ncel Haber {i+1}: {title}\nURL: {url}\nÄ°Ã§erik: {content_preview}...\n")
                else:
                    content_summary.append(f"GÃ¼ncel Haber {i+1}: {title}\nURL: {url}\nÃ–zet: {snippet}\n")
                
                # TÄ±klanabilir URL'ler iÃ§in baÅŸlÄ±k ve URL eÅŸleÅŸtirmesi
                source_urls_with_titles.append(f"ğŸ“° {title}\nğŸ”— {url}")
            
            # Gemini ile kapsamlÄ± analiz yap
            analysis_prompt = f"""
Sen profesyonel bir finans analisti olarak KCHOL hisse senedi fiyat tahmini yapÄ±yorsun.

KULLANICI SORUSU: {user_question}

MODEL TAHMÄ°NÄ°:
- Mevcut fiyat: {model_prediction.get('current_price', 'N/A')} TL
- Tahmin edilen fiyat: {model_prediction.get('predicted_price', 'N/A')} TL
- DeÄŸiÅŸim: {model_prediction.get('change', 0):+.2f} TL ({model_prediction.get('change_percent', 0):+.2f}%)
- Tahmin trendi: {prediction_trend}
- Tahmin gÃ¼cÃ¼: {prediction_strength:.2f}%

Ã‡ELÄ°ÅKÄ° ANALÄ°ZÄ°:
- Ã‡eliÅŸki var mÄ±: {has_conflict}
- Ã‡eliÅŸki aÃ§Ä±klamasÄ±: {conflict_explanation}

GÃœNCEL WEB Ä°Ã‡ERÄ°KLERÄ°:
{chr(10).join(content_summary)}

KAYNAK HABERLER:
{chr(10).join(source_urls_with_titles)}

ANALÄ°Z KURALLARI:
1. Sadece TÃ¼rkÃ§e yanÄ±t ver
2. Emoji kullanma
3. DÃ¼zyazÄ± ÅŸeklinde yaz
4. KÄ±sa ve Ã¶z ol (maksimum 5-6 paragraf)
5. Teknik jargon kullanma, anlaÅŸÄ±lÄ±r dil kullan
6. Model tahminini ve GÃœNCEL web haberlerini birlikte deÄŸerlendir
7. Ã‡eliÅŸki varsa nedenini aÃ§Ä±kla
8. Risk uyarÄ±sÄ± ekle
9. YatÄ±rÄ±m tavsiyesi verme, sadece analiz sun
10. GÃœNCEL haberlerin etkisini vurgula
11. Kaynak haberleri belirt ve tÄ±klanabilir URL'leri ver
12. "BugÃ¼n KCHOL niye dÃ¼ÅŸtÃ¼?" sorusu iÃ§in Ã¶zel olarak dÃ¼ÅŸÃ¼ÅŸ nedenlerini aÃ§Ä±kla

Ã–ZEL DURUMLAR:
- EÄŸer kullanÄ±cÄ± "niye dÃ¼ÅŸtÃ¼" diye soruyorsa, dÃ¼ÅŸÃ¼ÅŸ nedenlerini GÃœNCEL haberlerle destekle
- EÄŸer kullanÄ±cÄ± "niye yÃ¼kseldi" diye soruyorsa, yÃ¼kseliÅŸ nedenlerini GÃœNCEL haberlerle destekle
- Web haberlerinin model tahminini destekleyip desteklemediÄŸini belirt
- Haberlerin tarihini ve gÃ¼ncelliÄŸini vurgula
- Kaynak haberlerin URL'lerini belirt

YanÄ±tÄ±nÄ± ver:
"""
            
            if self.gemini_model:
                response = self.gemini_model.generate_content(analysis_prompt)
                analysis = response.text.strip()
            else:
                analysis = self._create_fallback_analysis(user_question, model_prediction, ranked_results, has_conflict, conflict_explanation)
            
            return {
                'success': True,
                'query': user_question,
                'model_prediction': model_prediction,
                'web_results_count': len(ranked_results),
                'web_results': ranked_results[:8],
                'analysis': analysis,
                'has_conflict': has_conflict,
                'conflict_explanation': conflict_explanation,
                'source_urls': source_urls_with_titles
            }
            
        except Exception as e:
            self.logger.error(f"Fiyat tahmini analizi hatasÄ±: {e}")
            return {
                'success': False,
                'message': f'Fiyat tahmini analizi hatasÄ±: {str(e)}',
                'model_prediction': model_prediction
            }
    
    def _create_fallback_analysis(self, user_question, model_prediction, search_results, has_conflict, conflict_explanation):
        """Gemini olmadÄ±ÄŸÄ±nda fallback analiz oluÅŸtur"""
        try:
            current_price = model_prediction.get('current_price', 0)
            predicted_price = model_prediction.get('predicted_price', 0)
            change = model_prediction.get('change', 0)
            change_percent = model_prediction.get('change_percent', 0)
            
            # KullanÄ±cÄ± sorusunu analiz et
            is_why_question = any(word in user_question.lower() for word in ['niye', 'neden', 'why', 'sebep', 'gerekÃ§e'])
            is_today_question = any(word in user_question.lower() for word in ['bugÃ¼n', 'today', 'gÃ¼ncel', 'son'])
            is_fall_question = any(word in user_question.lower() for word in ['dÃ¼ÅŸtÃ¼', 'dÃ¼ÅŸÃ¼ÅŸ', 'fall', 'dÃ¼ÅŸer', 'dÃ¼ÅŸecek'])
            
            # Trend analizi
            if change > 0:
                trend_analysis = f"Teknik model analizi, KCHOL hisse senedinin {predicted_price} TL seviyesine {change:+.2f} TL ({change_percent:+.2f}%) yÃ¼kseliÅŸle ulaÅŸmasÄ±nÄ± Ã¶ngÃ¶rÃ¼yor."
            elif change < 0:
                trend_analysis = f"Teknik model analizi, KCHOL hisse senedinin {predicted_price} TL seviyesine {change:+.2f} TL ({change_percent:+.2f}%) dÃ¼ÅŸÃ¼ÅŸle ulaÅŸmasÄ±nÄ± Ã¶ngÃ¶rÃ¼yor."
            else:
                trend_analysis = f"Teknik model analizi, KCHOL hisse senedinin {predicted_price} TL seviyesinde sabit kalmasÄ±nÄ± Ã¶ngÃ¶rÃ¼yor."
            
            # Web haber analizi
            web_analysis = ""
            source_urls = ""
            if search_results:
                web_analysis = f"\n\nGÃœNCEL HABER ANALÄ°ZÄ°:\n"
                source_urls = f"\n\nKAYNAK HABERLER:\n"
                
                for i, result in enumerate(search_results[:5], 1):
                    title = result.get('title', '')
                    url = result.get('url', '')
                    web_analysis += f"{i}. {title}\n"
                    source_urls += f"ğŸ“° {title}\nğŸ”— {url}\n\n"
                
                web_analysis += f"\nSon geliÅŸmeler ve gÃ¼ncel haberler analiz edildi."
            
            # Ã–zel durum analizi
            special_analysis = ""
            if is_why_question and is_today_question and is_fall_question:
                special_analysis = f"\n\nDÃœÅÃœÅ NEDENLERÄ° ANALÄ°ZÄ°:\n"
                special_analysis += "GÃ¼ncel haberler ve teknik analiz sonuÃ§larÄ±na gÃ¶re KCHOL hisse senedinin dÃ¼ÅŸÃ¼ÅŸ nedenleri:\n"
                special_analysis += "â€¢ Piyasa koÅŸullarÄ±ndaki genel dalgalanmalar\n"
                special_analysis += "â€¢ HissedarlarÄ±n kar satÄ±ÅŸÄ±na geÃ§mesi\n"
                special_analysis += "â€¢ Teknik indikatÃ¶rlerde aÅŸÄ±rÄ± alÄ±m sinyalleri\n"
                special_analysis += "â€¢ KÄ±sa vadeli dÃ¼zeltme ihtiyacÄ±\n"
                special_analysis += "â€¢ Genel piyasa risk algÄ±sÄ±ndaki deÄŸiÅŸimler\n"
            
            # Ã‡eliÅŸki aÃ§Ä±klamasÄ±
            conflict_analysis = ""
            if has_conflict:
                if change < 0:
                    conflict_analysis = f"\n\nÃ‡ELÄ°ÅKÄ° AÃ‡IKLAMASI:\nKullanÄ±cÄ± yÃ¼kseliÅŸ beklerken model dÃ¼ÅŸÃ¼ÅŸ tahmini yapÄ±yor. Bu durumun nedenleri:\n"
                    conflict_analysis += "â€¢ Teknik indikatÃ¶rler dÃ¼ÅŸÃ¼ÅŸ sinyali veriyor\n"
                    conflict_analysis += "â€¢ Piyasa koÅŸullarÄ± olumsuz gÃ¶rÃ¼nÃ¼yor\n"
                    conflict_analysis += "â€¢ KÄ±sa vadeli dÃ¼zeltme bekleniyor\n"
                    conflict_analysis += "â€¢ GÃ¼ncel haberler dÃ¼ÅŸÃ¼ÅŸ trendini destekliyor\n"
                else:
                    conflict_analysis = f"\n\nÃ‡ELÄ°ÅKÄ° AÃ‡IKLAMASI:\nKullanÄ±cÄ± dÃ¼ÅŸÃ¼ÅŸ beklerken model yÃ¼kseliÅŸ tahmini yapÄ±yor. Bu durumun nedenleri:\n"
                    conflict_analysis += "â€¢ Teknik indikatÃ¶rler yÃ¼kseliÅŸ sinyali veriyor\n"
                    conflict_analysis += "â€¢ Piyasa koÅŸullarÄ± olumlu gÃ¶rÃ¼nÃ¼yor\n"
                    conflict_analysis += "â€¢ Momentum devam ediyor\n"
                    conflict_analysis += "â€¢ GÃ¼ncel haberler yÃ¼kseliÅŸ trendini destekliyor\n"
            
            # Risk uyarÄ±sÄ±
            risk_warning = "\n\nRÄ°SK UYARISI:\nBu analiz teknik model ve gÃ¼ncel web haberlerine dayanmaktadÄ±r. YatÄ±rÄ±m kararÄ± vermeden Ã¶nce profesyonel danÄ±ÅŸmanlÄ±k almanÄ±zÄ± Ã¶neririm. GeÃ§miÅŸ performans gelecekteki sonuÃ§larÄ±n garantisi deÄŸildir."
            
            return f"KCHOL HÄ°SSE SENEDÄ° KAPSAMLI ANALÄ°Z\n\n{trend_analysis}{web_analysis}{special_analysis}{conflict_analysis}{source_urls}{risk_warning}"
            
        except Exception as e:
            self.logger.error(f"Fallback analiz hatasÄ±: {e}")
            return f"KCHOL hisse senedi analizi tamamlandÄ±. Teknik model tahmini: {change:+.2f} TL ({change_percent:+.2f}%). Risk uyarÄ±sÄ±: Bu analiz sadece bilgilendirme amaÃ§lÄ±dÄ±r."

# Test fonksiyonu
if __name__ == "__main__":
    agent = WebSearchAgent()
    
    # Test aramasÄ±
    result = agent.search_and_analyze("KCHOL hisse senedi son durumu")
    print(json.dumps(result, indent=2, ensure_ascii=False)) 